{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5lTujQ8pMe8XV2b3HY7f9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonhnlee/EDUC-5913-Programming-fundamentals-of-Python/blob/main/HW_Week12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigating the Top 30 Most Frequent Words in Disfluency Research\n",
        "\n",
        "In this exercise, I investigated the 30 most frequent words used in disfluency research.\n",
        "\n",
        "## Methods\n",
        "\n",
        " I downloaded two datasets from Scopus: one containing all articles from the search query \"disfluency AND linguistic OR language,\" and another containing \"disfluency AND linguistic OR language AND stuttering OR patient OR pathology.\" Since my focus was on non-stuttering disfluency research, I excluded all stuttering-related articles by comparing the DOIs of each document in the two datasets and removing any overlapping entries. After this filtering process, I used spaCy to analyze the abstracts of the remaining non-stuttering disfluency articles and identified the 30 most frequently occurring words."
      ],
      "metadata": {
        "id": "HkqI1XZFvbEN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oA6RisoBqgzz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data sets\n",
        "abstract_df = pd.read_excel('disfluency_abstract.xlsx')\n",
        "stuttering_df = pd.read_excel('disfluency_abstract.xlsx')"
      ],
      "metadata": {
        "id": "V4T1Cl_ds89I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whitespace in DOI column\n",
        "abstract_df['DOI'] = abstract_df['DOI'].str.strip().str.lower()\n",
        "stuttering_df['DOI'] = stuttering_df['DOI'].str.strip().str.lower()\n",
        "\n",
        "# Identify DOIs to exclude\n",
        "dois_to_exclude = set(stuttering_df['DOI'].dropna())  # Remove any NaN values\n",
        "\n",
        "# Exclude abstract_df where DOI matches any in stuttering_df\n",
        "df = abstract_df[~abstract_df['DOI'].isin(dois_to_exclude)]"
      ],
      "metadata": {
        "id": "MF-JtbKvuo3c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the abstracts of all remaining articles\n",
        "abstracts = df[\"Abstract\"].dropna().tolist()  # Drop NaN values and convert to list\n",
        "text = \" \".join(abstracts)  # Combine all abstracts into a single string"
      ],
      "metadata": {
        "id": "oWmbgLIaqyDq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprossing\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)  # Process the combined text with spaCy\n",
        "# Filter tokens\n",
        "filtered_tokens = [\n",
        "    token.lemma_.lower() for token in doc\n",
        "    if not token.is_stop and not token.is_punct and token.lemma_.isalpha()\n",
        "]"
      ],
      "metadata": {
        "id": "75LJ8FzfrtOQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word frequency\n",
        "word_freq = Counter(filtered_tokens)"
      ],
      "metadata": {
        "id": "tZ0u7ATNtMmR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 30 most frequent words\n",
        "top_30 = word_freq.most_common(30)"
      ],
      "metadata": {
        "id": "8DT3oAa-tO6U"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "tVFLRqKBxWSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 30 most frequent words and their frequencies:\")\n",
        "for word, freq in top_30:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVqxjSfMtRhb",
        "outputId": "592a0e2b-1356-4e1e-b411-0a9f865be423"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 most frequent words and their frequencies:\n",
            "speech: 711\n",
            "disfluency: 647\n",
            "language: 379\n",
            "model: 291\n",
            "word: 227\n",
            "system: 176\n",
            "base: 172\n",
            "paper: 169\n",
            "spontaneous: 151\n",
            "result: 148\n",
            "speaker: 145\n",
            "detection: 139\n",
            "task: 133\n",
            "feature: 128\n",
            "spoken: 127\n",
            "text: 119\n",
            "study: 117\n",
            "datum: 115\n",
            "recognition: 113\n",
            "approach: 111\n",
            "corpus: 111\n",
            "pause: 99\n",
            "performance: 98\n",
            "error: 93\n",
            "utterance: 92\n",
            "type: 92\n",
            "sentence: 89\n",
            "present: 87\n",
            "analysis: 86\n",
            "use: 84\n"
          ]
        }
      ]
    }
  ]
}